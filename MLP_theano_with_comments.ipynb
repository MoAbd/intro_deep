{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "(50000, 784) (50000,)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n",
      "--------------\n",
      "Example input:\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01171875\n",
      "  0.0703125   0.0703125   0.0703125   0.4921875   0.53125     0.68359375\n",
      "  0.1015625   0.6484375   0.99609375  0.96484375  0.49609375  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.1171875   0.140625    0.3671875\n",
      "  0.6015625   0.6640625   0.98828125  0.98828125  0.98828125  0.98828125\n",
      "  0.98828125  0.87890625  0.671875    0.98828125  0.9453125   0.76171875\n",
      "  0.25        0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.19140625\n",
      "  0.9296875   0.98828125  0.98828125  0.98828125  0.98828125  0.98828125\n",
      "  0.98828125  0.98828125  0.98828125  0.98046875  0.36328125  0.3203125\n",
      "  0.3203125   0.21875     0.15234375  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.0703125   0.85546875  0.98828125  0.98828125  0.98828125\n",
      "  0.98828125  0.98828125  0.7734375   0.7109375   0.96484375  0.94140625\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.3125      0.609375\n",
      "  0.41796875  0.98828125  0.98828125  0.80078125  0.04296875  0.\n",
      "  0.16796875  0.6015625   0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.0546875   0.00390625  0.6015625   0.98828125  0.3515625   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.54296875  0.98828125  0.7421875   0.0078125   0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.04296875  0.7421875   0.98828125  0.2734375   0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.13671875  0.94140625  0.87890625\n",
      "  0.625       0.421875    0.00390625  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.31640625\n",
      "  0.9375      0.98828125  0.98828125  0.46484375  0.09765625  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.17578125  0.7265625   0.98828125  0.98828125  0.5859375   0.10546875\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.0625      0.36328125  0.984375    0.98828125\n",
      "  0.73046875  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.97265625\n",
      "  0.98828125  0.97265625  0.25        0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.1796875   0.5078125   0.71484375\n",
      "  0.98828125  0.98828125  0.80859375  0.0078125   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.15234375  0.578125    0.89453125  0.98828125\n",
      "  0.98828125  0.98828125  0.9765625   0.7109375   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.09375     0.4453125   0.86328125  0.98828125  0.98828125\n",
      "  0.98828125  0.98828125  0.78515625  0.3046875   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.08984375  0.2578125   0.83203125  0.98828125  0.98828125  0.98828125\n",
      "  0.98828125  0.7734375   0.31640625  0.0078125   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.0703125\n",
      "  0.66796875  0.85546875  0.98828125  0.98828125  0.98828125  0.98828125\n",
      "  0.76171875  0.3125      0.03515625  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.21484375\n",
      "  0.671875    0.8828125   0.98828125  0.98828125  0.98828125  0.98828125\n",
      "  0.953125    0.51953125  0.04296875  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.53125     0.98828125  0.98828125  0.98828125  0.828125    0.52734375\n",
      "  0.515625    0.0625      0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "Example label:\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "# First, let's load our data and take a look!\n",
    "import pickle\n",
    "\n",
    "# Load our data \n",
    "# Download and unzip pickled version from here: http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = pickle.load(open('data/mnist.pkl', 'r'))\n",
    "print \"Shapes:\"\n",
    "print train_x.shape, train_y.shape\n",
    "print valid_x.shape, valid_y.shape\n",
    "print test_x.shape, test_y.shape\n",
    "\n",
    "print \"--------------\"\n",
    "print \"Example input:\"\n",
    "print train_x[0]\n",
    "print \"Example label:\"\n",
    "print train_y[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "encoder zip not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fa1cb89a7093>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m                        tile_spacing=(1, 1))\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"some_mnist_numbers.png\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"some_mnist_numbers.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1693\u001b[1;33m             \u001b[0msave_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1695\u001b[0m             \u001b[1;31m# do what we can to clean up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/PIL/PngImagePlugin.pyc\u001b[0m in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, check)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     ImageFile._save(im, _idat(fp, chunk),\n\u001b[1;32m--> 757\u001b[1;33m                     [(\"zip\", (0, 0)+im.size, 0, rawmode)])\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m     \u001b[0mchunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb\"IEND\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mb\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/PIL/ImageFile.pyc\u001b[0m in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[1;31m# compress to Python file-compatible object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 476\u001b[1;33m             \u001b[0me\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoderconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    477\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mo\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                 \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36m_getencoder\u001b[1;34m(mode, encoder_name, args, extra)\u001b[0m\n\u001b[0;32m    434\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"encoder %s not available\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mencoder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: encoder zip not available"
     ]
    }
   ],
   "source": [
    "# Show example images - using tile_raster_images helper function from OpenDeep to get 28x28 image from 784 array.\n",
    "from utils import tile_raster_images\n",
    "from PIL import Image as pil_img\n",
    "\n",
    "input_images = train_x[:25]\n",
    "im = pil_img.fromarray(\n",
    "    tile_raster_images(input_images, \n",
    "                       img_shape=(28, 28), \n",
    "                       tile_shape=(1, 25),\n",
    "                       tile_spacing=(1, 1))\n",
    ")\n",
    "im.save(\"some_mnist_numbers.png\")\n",
    "Image(filename=\"some_mnist_numbers.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cool, now we know a little about the input data, let's design the MLP to work with it!\n",
    "\n",
    "# Your basic Theano imports.\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "# Inputs are matrices where rows are examples and columns are pixels - so create a symbolic Theano matrix.\n",
    "x = T.matrix('x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's start building the equation for our MLP!\n",
    "\n",
    "# The first transformation is the input x -> hidden layer h.\n",
    "# We defined this transformation with h = tanh(x.dot(W_x) + b_h)\n",
    "# where the learnable model parameters are W_x and b_h.\n",
    "\n",
    "# Therefore, we will need a weights matrix W_x and a bias vector b_h.\n",
    "# W_x has shape (input_size, hidden_size) and b_h has shape (hidden_size,).\n",
    "# Initialization is important in deep learning; we want something random so the model doesn't get stuck early.\n",
    "# Many papers in this subject, but for now we will just use a normal distribution with mean=0 and std=0.05.\n",
    "# Another good option for tanh layers is to use a uniform distribution with interval +- sqrt(6/sum(shape)).\n",
    "# These are hyperparameters to play with.\n",
    "# Bias starting as zero is fine.\n",
    "import numpy.random as rng\n",
    "W_x = numpy.asarray(rng.normal(loc=0.0, scale=.05, size=(28*28, 500)), dtype=\"float32\")\n",
    "b_h = numpy.zeros(shape=(500,), dtype=\"float32\")\n",
    "\n",
    "# To update a variable used in an equation (for example, while learning), \n",
    "# Theano needs it to be in a special wrapper called a shared variable.\n",
    "# These are the model parameters for our first hidden layer!\n",
    "W_x = theano.shared(W_x, name=\"W_x\")\n",
    "b_h = theano.shared(b_h, name=\"b_h\")\n",
    "\n",
    "# Now, we can finally write the equation to give our symbolic hidden layer h!\n",
    "h = T.tanh(\n",
    "    T.dot(x, W_x) + b_h\n",
    ")\n",
    "\n",
    "# Side note - if we used softmax instead of tanh for the activation, this would be performing logistic regression!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have the hidden layer h, let's put that softmax layer on top for classification output y!\n",
    "\n",
    "# Same deal as before, the transformation is defined as:\n",
    "# y = softmax(h.dot(W_h) + b_y)\n",
    "# where the learnable parameters are W_h and b_y.\n",
    "# W_h has shape (hidden_size, output_size) and b_y has shape (output_size,).\n",
    "\n",
    "# We will use the same random initialization strategy as before.\n",
    "W_h = numpy.asarray(rng.normal(loc=0.0, scale=.05, size=(500, 10)), dtype=\"float32\")\n",
    "b_y = numpy.zeros(shape=(10,), dtype=\"float32\")\n",
    "# Don't forget to make them shared variables!\n",
    "W_h = theano.shared(W_h, name=\"W_h\")\n",
    "b_y = theano.shared(b_y, name=\"b_y\")\n",
    "\n",
    "# Now write the equation for the output!\n",
    "y = T.nnet.softmax(\n",
    "    T.dot(h, W_h) + b_y\n",
    ")\n",
    "\n",
    "# The output (due to softmax) is a vector of class probabilities.\n",
    "# To get the output class 'guess' from the model, just take the index of the highest probability!\n",
    "y_hat = T.argmax(y, axis=1)\n",
    "\n",
    "# That's everything! Just four model parameters and one input variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The variable y_hat represents the output of running our model, but we need a cost function to use for training.\n",
    "# For a softmax (probability) output, we want to maximize the likelihood of P(Y=y|X).\n",
    "# This means we want to minimize the negative log-likelihood cost! (For a primer, see machine learning Coursera.)\n",
    "\n",
    "# Cost functions always need the truth outputs to compare against (this is supervised learning).\n",
    "# From before, we saw the labels were a vector of ints - so let's make a symbolic variable for this!\n",
    "correct_labels = T.ivector(\"labels\")  # integer vector\n",
    "\n",
    "# Now we can compare our output probability from y with the true labels.\n",
    "# Because the labels are integers, we will want to make an indexing mask to pick out the probabilities\n",
    "# our model thought was the likelihood of the correct label.\n",
    "correct_indices = []\n",
    "log_likelihood = T.log(y)[T.arange(correct_labels.shape[0]), correct_labels]\n",
    "# We use mean instead of sum to be less dependent on batch size (better for flexibility)\n",
    "cost = -T.mean(log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Easiest way to train neural nets is with Stochastic Gradient Descent\n",
    "# This takes each example, calculates the gradient, and changes the model parameters a small amount\n",
    "# in the direction of the gradient.\n",
    "\n",
    "# Fancier add-ons to stochastic gradient descent will reduce the learning rate over time, add a momentum\n",
    "# factor to the parameters, etc.\n",
    "\n",
    "# Before we can start training, we need to know what the gradients are.\n",
    "# Luckily we don't have to do any math! Theano has symbolic auto-differentiation which means it can\n",
    "# calculate the gradients for arbitrary equations with respect to a cost and parameters.\n",
    "parameters = [W_x, b_h, W_h, b_y]\n",
    "gradients = T.grad(cost, parameters)\n",
    "# Now gradients contains the list of derivatives: [d_cost/d_W_x, d_cost/d_b_h, d_cost/d_W_h, d_cost/d_b_y]\n",
    "\n",
    "# One last thing we need to do before training is to use these gradients to update the parameters!\n",
    "# Remember how parameters are shared variables? Well, Theano uses something called updates\n",
    "# which are just pairs of (shared_variable, new_variable_expression) to change its value.\n",
    "# So, let's create these updates to show how we change the parameter values during training with gradients!\n",
    "# We use a learning rate to make small steps over time.\n",
    "learning_rate = 0.01\n",
    "train_updates = [(param, param - learning_rate*gradient) for param, gradient in zip(parameters, gradients)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can create a Theano function that takes in real inputs and trains our model.\n",
    "f_train = theano.function(inputs=[x, correct_labels], outputs=cost, updates=train_updates, allow_input_downcast=True)\n",
    "\n",
    "# For testing purposes, we don't want to use updates to change the parameters - so create a separate function!\n",
    "# We also care more about the output guesses, so let's return those instead of the cost.\n",
    "# error = sum(T.neq(y_hat, correct_labels))/float(y_hat.shape[0])\n",
    "f_test = theano.function(inputs=[x], outputs=y_hat, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our training can begin!\n",
    "# The two hyperparameters we have for this part are minibatch size (how many examples to process in parallel)\n",
    "# and the total number of passes over all examples (epochs).\n",
    "batch_size = 100\n",
    "epochs = 30\n",
    "\n",
    "# Given our batch size, compute how many batches we can fit into each data set\n",
    "train_batches = len(train_x) / batch_size\n",
    "valid_batches = len(valid_x) / batch_size\n",
    "test_batches = len(test_x) / batch_size\n",
    "\n",
    "# Theano works a lot faster on the GPU - and one bottleneck is i/o transferring data.\n",
    "# One way around this is to load the datasets in memory first as shared variables - keeps it on the GPU.\n",
    "# train_x = theano.shared(train_x)\n",
    "# train_y = theano.shared(train_y)\n",
    "# valid_x = theano.shared(valid_x)\n",
    "# valid_y = theano.shared(valid_y)\n",
    "# test_x = theano.shared(test_x)\n",
    "# test_y = theano.shared(test_y)\n",
    "\n",
    "# Our main training loop!\n",
    "for epoch in range(epochs):\n",
    "    print epoch+1, \":\",\n",
    "    \n",
    "    train_costs = []\n",
    "    train_accuracy = []\n",
    "    for i in range(train_batches):\n",
    "        batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_labels = train_y[i*batch_size:(i+1)*batch_size]\n",
    "        costs = f_train(batch_x, batch_labels)\n",
    "        preds = f_test(batch_x)\n",
    "        \n",
    "        acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "        \n",
    "        train_costs.append(costs)\n",
    "        train_accuracy.append(acc)\n",
    "    print \"cost:\", numpy.mean(train_costs), \"\\ttrain:\", str(numpy.mean(train_accuracy)*100)+\"%\",\n",
    "    \n",
    "    valid_accuracy = []\n",
    "    for i in range(valid_batches):\n",
    "        batch_x = valid_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_labels = valid_y[i*batch_size:(i+1)*batch_size]\n",
    "        preds = f_test(batch_x)\n",
    "        \n",
    "        acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "        valid_accuracy.append(acc)\n",
    "    print \"\\tvalid:\", str(numpy.mean(valid_accuracy)*100)+\"%\",\n",
    "    \n",
    "    test_accuracy = []\n",
    "    for i in range(test_batches):\n",
    "        batch_x = test_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_labels = test_y[i*batch_size:(i+1)*batch_size]\n",
    "        preds = f_test(batch_x)\n",
    "        \n",
    "        acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "        test_accuracy.append(acc)\n",
    "    print \"\\ttest:\", str(numpy.mean(test_accuracy)*100)+\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
